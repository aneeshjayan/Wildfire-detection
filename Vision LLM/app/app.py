import streamlit as st
from PIL import Image, ImageDraw
import torch
from torchvision import transforms
from transformers import AutoProcessor, AutoModelForVision2Seq
from crossvit import CrossViT

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load DL model (CrossViT) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_dl_model():
    model = CrossViT(image_size=224, channels=3, num_classes=2)
    state_dict = torch.load("C:/Users/HP ZBOOK FURY G7/Downloads/model_fold_5.pt", map_location="cpu")
    model.load_state_dict(state_dict)
    model.eval()
    return model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load Kosmos-2 LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_llm():
    model_id = "microsoft/kosmos-2-patch14-224"
    processor = AutoProcessor.from_pretrained(model_id)
    model = AutoModelForVision2Seq.from_pretrained(model_id)
    return processor, model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Classify image with DL model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def classify_image(image, model):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    tensor = transform(image).unsqueeze(0)
    with torch.no_grad():
        output = model(tensor)
    pred_class = torch.argmax(output, dim=1).item()
    confidence = torch.softmax(output, dim=1)[0][pred_class].item()
    return pred_class, confidence

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Draw Entity Boxes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def draw_boxes(image, entities):
    draw = ImageDraw.Draw(image)
    w, h = image.size
    for _, _, boxes in entities:
        for box in boxes:
            x1, y1, x2, y2 = [box[0]*w, box[1]*h, box[2]*w, box[3]*h]
            draw.rectangle([x1, y1, x2, y2], outline="red", width=2)
    return image

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agentic Explanation via LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def agent_explanation(image, pred_class, confidence, processor, llm_model, user_prompt=""):
    label = {0: "No Fire", 1: "Fire", 2: "Smoke"}.get(pred_class, "Unknown")
    prompt = (
        f"<grounding>This image was classified as '{label}' with {confidence*100:.2f}% confidence.\n"
        f"What visual patterns might support this prediction (e.g., smoke, flames, surroundings)?"
    )
    if user_prompt.strip():
        prompt += f"\n\n<user_question>{user_prompt.strip()}</user_question>"

    inputs = processor(text=prompt, images=image, return_tensors="pt")
    output_ids = llm_model.generate(
        pixel_values=inputs["pixel_values"],
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        image_embeds_position_mask=inputs["image_embeds_position_mask"],
        max_new_tokens=256,
    )
    decoded = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    explanation, entities = processor.post_process_generation(decoded)
    return explanation, entities

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ğŸ”¥ Agentic Fire Detector")
st.title("ğŸ§  Agentic AI - Fire/Smoke Detection + Custom Prompt")

image_file = st.file_uploader("ğŸ“· Upload an RGB image", type=["jpg", "jpeg", "png"])
user_prompt = st.text_area("ğŸ’¬ Ask a follow-up question (optional)", placeholder="e.g., Is the smoke spreading?")

if image_file:
    image = Image.open(image_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_container_width=True)

    with st.spinner("ğŸ” Running CrossViT classifier..."):
        dl_model = load_dl_model()
        pred_class, confidence = classify_image(image, dl_model)

    with st.spinner("ğŸ§  Running Kosmos-2 Vision LLM..."):
        processor, llm_model = load_llm()
        explanation, entities = agent_explanation(image, pred_class, confidence, processor, llm_model, user_prompt)

    st.success("âœ… Done!")
    st.markdown(f"### ğŸ“Œ Prediction:\n**Class:** `{pred_class}` | **Confidence:** `{confidence*100:.2f}%`")
    st.markdown("### ğŸ§  LLM Explanation")
    st.info(explanation)

    if entities:
        st.markdown("### ğŸ” Grounded Entities (Bounding Boxes)")
        st.image(draw_boxes(image.copy(), entities), caption="ğŸ“Œ Detected Regions", use_container_width=True)

        for text, _, boxes in entities:
            for box in boxes:
                st.markdown(f"- **{text}** â†’ [x1={box[0]:.2f}, y1={box[1]:.2f}, x2={box[2]:.2f}, y2={box[3]:.2f}]")
